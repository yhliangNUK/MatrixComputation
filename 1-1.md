# § 1.1 向量、矩陣、秩、正交、特殊矩陣、特徵值與特徵向量 (1)

# § 1.1.1 符號、基本定義與性質

Denote

$$
A \in \mathbb{K}^{m \times n}, \text{ where } \mathbb{K} = \mathbb{R} \text{ or } \mathbb{C}
\Leftrightarrow
A = [a_{ij}] = \begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix},
\quad a_{ij} \in \mathbb{K}.
$$

Then we define

1. **Product of Matrices $(\mathbb{K}^{m \times n} \times \mathbb{K}^{n \times p} \to \mathbb{K}^{m \times p})$  [乘積]**: Let $A\in \mathbb{K}^{m \times n}, B\in \mathbb{K}^{n \times p}$. Then $C=AB\in \mathbb{K}^{m \times p}$, and
    
    $$
    C=[c_{ij}]= AB \quad\Longrightarrow \quad c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj},\quad \forall\; i = 1, \ldots, m,\; j = 1, \ldots, p
    $$
    
2. **Transpose $(\mathbb{R}^{m \times n} \to \mathbb{R}^{n \times m} )$ [轉置]**: 
    
    $$
    C = A^T, \;\; \text{where}\;\; c_{ij} = a_{ji} \in \mathbb{R}
    $$
    
3. **Conjugate transpose $(\mathbb{C}^{m \times n} \to \mathbb{C}^{n \times m})$ [共軛轉置]**:  
    
    $$
    C = A^* \text{ or } C = A^H. \;\; \text{where} \;\; c_{ij} = \overline{a_{ji}} \in \mathbb{C}
    $$
    
4. **Differentiation $(\mathbb{R}^{m \times n} \to \mathbb{R}^{m \times n})$  [微分]**: 
    
    $$
    C(t) = [ c_{ij}(t) ] \quad \Longrightarrow \quad \dot{C}(t) = [\dot{c}_{ij}(t)]
    $$
    
5. **Inverse $(\mathbb{K}^{m \times n} \to \mathbb{K}^{m \times n})$**  **[反矩陣]**: If $A,B \in \mathbb{K}^{n \times n}$ satisfy $AB = I$, then $B$  is called the **inverse** of $A$  and is denoted by $A^{-1}$. 
    - If $A^{-1}$ exists, then $A$ is said to be **nonsingular [非奇異的]** or **invertible [可逆的]**; otherwise,  $A$ is **singular [奇異的]** or **not invertible [不可逆的]**.
    - It can be proved that $A$ is nonsingular $\Leftrightarrow$ $\det(A) \neq 0$.
6. **Inner product [內積]**: The inner product of $x,y \in \mathbb{K}^{n}$ is defined by
    
    $$
    \begin{align*}
    \langle x, y \rangle &:= x^T y = \sum_{i=1}^{n} x_i y_i = y^T x \in \mathbb{R},\\
    \langle x, y \rangle &:= x^* y = \sum_{i=1}^{n} \overline{x_i} y_i = y^* x \in \mathbb{C},
    \end{align*}
    $$
    
7. **Some basis operations**:
    - Let $A \in \mathbb{K}^{m \times n}$ and $x \in \mathbb{K}^{n}$. Then
        - Form 1:
            
            $$
            y= Ax \quad \Longrightarrow \quad y= [y_i],\;\; \text{where}\;\; y_i = \sum\limits_{j=1}^{n} a_{ij} x_j, \; i = 1, \ldots, m
            $$
            
        - Form 2:
            
            $$
            y= Ax = \begin{bmatrix} v_1 \mid v_2 \mid \cdots \mid v_n \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ v_n \end{bmatrix} 
             \quad \Longrightarrow \quad y= x_1 v_1+x_2 v_2+\cdots+x_n v_n.
            $$
            
        - Form 3:
            
            $$
            y= Ax = \begin{bmatrix} r_1\\ r_2\\ \vdots \\ r_m\end{bmatrix} x
             \quad \Longrightarrow \quad y= \begin{bmatrix} \langle r_1, x \rangle\\ \langle r_2, x \rangle\\ \vdots \\ \langle r_m, x \rangle\end{bmatrix}.
            $$
            
    - Let $x \in \mathbb{K}^{m}$ and $y \in \mathbb{K}^{n}$. Then
        
        $$
        xy^* = \begin{bmatrix}
        x_1 \overline{y_1} & \cdots & x_1 \overline{y_n} \\
        \vdots & \ddots & \vdots \\
        x_m \overline{y_1} & \cdots & x_m \overline{y_n}
        \end{bmatrix}
        \in \mathbb{K}^{m \times n}.
        $$
        

## 擾動下的反矩陣

### **Theorem (Sherman-Morrison Formula):**

Let $A \in \mathbb{R}^{n \times n}$ be nonsingular. Then, for any $u, v \in \mathbb{R}^n$, if $v^T A^{-1} u \neq -1$, then

$$
(A + uv^T)^{-1} = A^{-1} - \frac{A^{-1} u v^T A^{-1}}{1 + v^T A^{-1} u}.
$$

- Proof:
    
    $$
    \begin{align*}                    &(A + uv^T) [A^{-1} - A^{-1} u (1 + v^T A^{-1} u)^{-1} v^T A^{-1}] \\                    &= I + \frac{1}{1 + v^T A^{-1} u} [uv^T A^{-1} (1 + v^T A^{-1} u) - uv^T A^{-1} - uv^T A^{-1} uv^T A^{-1}] \\                    &= I + \frac{1}{1 + v^T A^{-1} u} [u (v^T A^{-1} u) v^T A^{-1} - uv^T A^{-1} uv^T A^{-1}] = I.            \end{align*}
    $$
    

### **Theorem (Sherman-Morrison-Woodbury Formula):**

Let $A \in \mathbb{R}^{n \times n}$ be nonsingular. Then, for any $U, V \in \mathbb{R}^{n \times k}$, if  $(I + V^T A^{-1} U)$ is invertible, then

$$
(A + UV^T)^{-1} = A^{-1} - A^{-1} U (I + V^T A^{-1} U)^{-1} V^T A^{-1}.
$$

**Example:**

$$
A = \begin{bmatrix}        3 & -1 & 1 & 1 & 1 \\        0 & 1 & 2 & 2 & 2 \\        0 & -1 & 4 & 1 & 1 \\        0 & 0 & 0 & 3 & 0 \\        0 & 0 & 0 & 0 & 3        \end{bmatrix} = B + \begin{bmatrix}        0 \\        0 \\        -1 \\        0 \\        0        \end{bmatrix} \begin{bmatrix}        0 & 1 & 0 & 0 & 0        \end{bmatrix}.
$$

# § 1.1.2 秩與正交

Let $A \in \mathbb{R}^{m \times n}$. Consider the mapping

$$
\begin{align*}
T: \mathbb{R}^n &\to \mathbb{R}^m \\
x &\to y=Ax 
\end{align*}
$$

### **Definition (Range, Null Space, Rank, Nullity):**

Let $A \in \mathbb{R}^{m \times n}$. Then we define

1. The **range space [值域]** of $A$ by
    
    $$
    R(A) = \{ y \in \mathbb{R}^{m} \mid y = Ax \text{ for some } x \in \mathbb{R}^{n} \} \subseteq \mathbb{R}^{m}.
    $$
    
2. The **null space/kernel [零空間/零核]** of $A$ by
    
    $$
    N(A) = \{ x \in \mathbb{R}^{n} \mid Ax = 0 \} \subseteq \mathbb{R}^{n}.
    $$
    
3. The **rank [秩]** of $A$ by
    
    $$
    \text{rank}(A) = \dim [R(A)] = \text{The number of maximal linearly independent columns of } A
    $$
    
4. The **nullity** of $A$ by
    
    $$
    \text{nullity}(A) = \dim [N(A)] = \text{The number of maximal linearly independent vectors of } N(A).
    $$
    

### **Theorem (Dimension Theorem):**

For any $A\in \mathbb{R}^{m\times n},$ $\text{nullity}(A) + \text{rank}(A) = n \text{ (column number)}$.

### **Theorem (Equivalence of Nonsingular):**

 Let $A\in \mathbb{R}^{n\times n}$.  Then the following are equivalent.

1. $A$ is nonsingular.
2. $Ax=0$ has only the solution $x=0$.
3. $Ax=b$ has a unique solution, for any $b\in\mathbb{R}^n$.
4. $N(A) = \{0\}$.
5. $R(A)= \mathbb{R}^n$.
6. $\text{rank}(A) = n$.
7. $\text{nullity}(A) = 0.$
8. $\det(A) \neq 0.$

### **Definition (Orthogonal, Orthonormal, Orthogonal Complement):**

1. Let $\{x_1, \ldots, x_p\} \subseteq \mathbb{R}^{n}$. Then $\{x_1, \ldots, x_p\}$ is said to be **orthogonal [正交]** if $x_i^T x_j = 0$, $\forall\; i \neq j$.
2. Let $\{x_1, \ldots, x_p\} \subseteq \mathbb{R}^{n}$. Then $\{x_1, \ldots, x_p\}$ is said to be **orthonormal [單範正交]** if $x_i^T x_j = 0$, $\forall\; i \neq j$, and  $x_i^T x_i = 1$, $\forall\; i$.
3. The **orthogonal complement [正交補]** of a set/space $S$ is defined by
    
    $$
    S^\perp = \{ y \in \mathbb{R}^{m} \mid y^T x = 0, \text{ for } x \in S \} = \text{orthogonal complement of } S.
    $$
    

### **Theorem (Relation between Range and Null Space of $A$ and $A^T$):**

1. For any $A\in \mathbb{R}^{m\times n},$ $R(A^T) \perp N(A)$ and $R(A) \perp N(A^T)$.
2. For any $A\in \mathbb{R}^{m\times n},$ $\text{rank}(A) = \text{rank}(A^T)$.

### **Definition ( <Skew> Symmetric/Hermitian, Definite, Normal, Orthogonal, Unitary):**

|  | $A \in \mathbb{R}^{n \times n}$ | $A \in \mathbb{C}^{n \times n}$ |
| --- | --- | --- |
| **[對稱]** | **Symmetric**: $A^T = A$ | **Hermitian**: $A^* = A (A^H = A)$ |
| **[反對稱]** | **Skew-symmetric**: $A^T = -A$ | **Skew-Hermitian**: $A^* = -A$ |
| **Positive definite [正定]** | $x^T A x > 0$, $x \neq 0$ | $x^* A x > 0$, $x \neq 0$ |
| **Non-negative definite [非負定]** | $x^T A x \geq 0$ | $x^* Ax \geq 0$ |
| **Indefinite [不定]** | $(x^T A x)(y^T A y) < 0$ for some $x, y$ | $(x^* A x)(y^* A y) < 0$ for some $x, y$ |
| **Normal [正規]** | $A^T A = A A^T$ | $A^* A = A A^*$ |
| **[正交]** | **Orthogonal [正交]**: $A^TA=AA^T= I$ | **Unitary [么正]**: $A^* A = A A^* = I$  |

**Example:**

If $A=[a_{ij}]\in \mathbb{R}^{n\times n}$ is skew-symmetric, then show that

1. $a_{ii}=0$ for all $i=1,\ldots,n.$
2. $x^TAx=0$ for all $x\in\mathbb{R}^n$.

# § 1.1.3 特殊矩陣

### **Definition (Diagonal, Upper/Lower Triangular, Tridiagonal, Upper/Lower Bi-Diagonal, Upper/ Lower Hessenberg, Sparse):**

Let $A \in \mathbb{R}^{m \times n}$. Then we say the matrix $A$ is

1. **diagonal [對角的]** if  $a_{ij} = 0$ for all $i \neq j$. Sometimes, to save the notations, we denote 
    
    $$
     D =\begin{bmatrix} d_1 &0 &\cdots &\cdots  &0 \\ 0 &d_2 & 0 &\cdots &0\\ \vdots &\ddots &\ddots &\ddots &\vdots \\ \vdots &\ddots &\ddots & d_{n-1} & 0\\ 0 &\cdots &\cdots & 0 & d_n  \end{bmatrix}= \text{diag}(d_1, \ldots, d_n)
    $$
    
2. **(strictly) upper triangular [(嚴格)上三角]** if $a_{ij} = 0$ for $i > j$ (or $i \geq j$);
    
    $$
    \begin{bmatrix}
    a_{11} & a_{12} & a_{13} & a_{14} \\
    0 & a_{22} & a_{23} & a_{24} \\
    0 & 0 & a_{33} & a_{34} \\
    0 & 0 & 0 & a_{44}
    \end{bmatrix}, \;\;
    \begin{bmatrix}
    0 & a_{12} & a_{13} & a_{14} \\
    0 & 0 & a_{23} & a_{24} \\
    0 & 0 & 0 & a_{34} \\
    0 & 0 & 0 & 0
    \end{bmatrix}
    $$
    
3. **(strictly) lower triangular [(嚴格)下三角]** if $a_{ij} = 0$ for $i < j$ (or $i \leq j$);
    
    $$
    \begin{bmatrix}
    a_{11} & 0 & 0 & 0 \\
    a_{21} & a_{22} & 0 & 0 \\
    a_{31} & a_{32} & a_{33} & 0 \\
    a_{41} & a_{42} & a_{43} & a_{44}
    \end{bmatrix},\;\;
    \begin{bmatrix}
    0 & 0 & 0 & 0 \\
    a_{21} & 0 & 0 & 0 \\
    a_{31} & a_{32} & 0 & 0 \\
    a_{41} & a_{42} & a_{43} & 0
    \end{bmatrix}
    $$
    
4. **tridiagonal [三對角的]** if $a_{ij} = 0$ ＝for  $|i-j| > 1$.
    
    $$
    \begin{bmatrix}
    a_{11} & a_{12} & 0 & 0 \\
    a_{21} & a_{22} & a_{23} & 0 \\
    0 & a_{32} & a_{33} & a_{34} \\
    0 & 0 & a_{43} & a_{44}
    \end{bmatrix}
    $$
    
5. **upper bi-diagonal [上雙對角的]** if $a_{ij} = 0$ for  $i > j$ or $j > i+1$.
    
    $$
    \begin{bmatrix}
    a_{11} & a_{12} & 0 & 0 \\
    0 & a_{22} & a_{23} & 0 \\
    0 & 0 & a_{33} & a_{34} \\
    0 & 0 & 0 & a_{44}
    \end{bmatrix}
    $$
    
6. **upper Hessenberg [上海森伯格]** if $a_{ij} = 0$ for $i > j+1$. (Note: the lower case is the same as above.)
    
    $$
    \begin{bmatrix}
    a_{11} & a_{12} & a_{13} & a_{14} \\
    a_{21} & a_{22} & a_{23} & a_{24} \\
    0 & a_{32} & a_{33} & a_{34} \\
    0 & 0 & a_{43} & a_{44}
    \end{bmatrix}
    $$
    

 7. **sparse [稀疏矩陣]** if at most $n^{1+r}$ elements of $A \in \mathbb{R}^{m \times n}$ is nonzero, where $r < 1$ (usually between $0.2$ and $0.5$).

- For example,  If $n = 1000, r = 0.9$, then $n^{1+r} = 501,187$.

**Example:**

If $S$ is skew-symmetric, then $I - S$ is nonsingular and $(I - S)^{-1} (I + S)$ is orthogonal (**Cayley transformation** of $S$).

- Solution
    1. $S=[s_{ij}]$ satisfies $s_{ij}= - s_{ji}$. So $s_{ii}=0$ for all $i$.
    2. $x^TSx=0$ for all $x\in \mathbb{R}^n$.
    3. A matrix $A$ is singular $\Rightarrow$ $Av=0$ for some $v\neq 0$.

# § 1.1.4 特徵值特徵、向量

### **Definition (Eigenvalues and Eigenvectors):**

Let $A \in \mathbb{C}^{n \times n}$. Then $\lambda \in \mathbb{C}$ is called an **eigenvalue [特徵值]** of $A$ if there exists $x \neq 0, x \in \mathbb{C}^n$ such that 

$$
Ax = \lambda x
$$

and $x$ is called an **eigenvector [特徵向量]** of $A$ corresponding to $\lambda$. We also call $(\lambda, x)$ be an **eigenpair [特徵對]** of $A$.

### **Definition (Spectrum and Spectrum Radius):**

Let $A \in \mathbb{C}^{n \times n}$. Then we define

1. $\sigma(A) := \text{Spectrum (譜) of } A = \text{The set of eigenvalues of } A$.
2. $\rho(A) := \text{Sprctral Radius (譜半徑) of } A = \max \{ |\lambda| : \lambda \in \sigma(A) \}$.

### **Process (To find (Exact) Eigenvalues and Eigenvectors of a Matrix):**

Let $A \in \mathbb{C}^{n \times n}$. 

1. Note that  $\lambda \in \sigma(A) \Leftrightarrow \det(A - \lambda I) = 0$ .
2. Define the **characteristic polynomial [特徵多項式]** of $A$ by $p(\lambda) = \det(\lambda I - A)$.
3. Factor
    
    $$
    p(\lambda) = \prod_{i=1}^{s} (\lambda - \lambda_i)^{m(\lambda_i)}
    $$
    
    where $\lambda_i \neq \lambda_j$ (for $i \neq j$) and $\sum\limits_{i=1}^{s} m(\lambda_i) = n$.
    
4. Then the set of **eigenvalues** of $A$ is $\sigma(A) = \{ \lambda_1,\lambda_2, \cdots, \lambda_s \}$ .
5. Note that $Ax = \lambda x \Leftrightarrow  (A-\lambda I) x = 0$.
6. For each eigenvalue $\lambda_i, i=1,\ldots,s,$ the set that collects all its corresponding **eigenvectors** is
    
    $$
    \{ v \neq 0 \mid (A-\lambda_i I) v =0 \}= \text{null}( A-\lambda_i I )- \{ 0 \}.
    $$
    

### **Definition (Algebraic Multiplicity and Geometric Multiplicity):**

Let $A \in \mathbb{C}^{n \times n}$ and suppose the characteristic polynomial $p(\lambda)$ of $A$ is 

$$
p(\lambda) = \prod_{i=1}^{s} (\lambda - \lambda_i)^{m(\lambda_i)}
$$

Then, for each eigenvalue of $\lambda_i, i=1,\ldots,s,$ we say

1. $m(\lambda_i) =$  The **algebraic multiplicity [代數重數]** of $\lambda_i$.
2. $n(\lambda_i) = \text{nullity}(A - \lambda_i I) =$ The **geometric multiplicity [幾何重數]** of $\lambda_i$.

### **Theorem (Relation between Algebraic and Geometric Multiplicities):**

Let $A \in \mathbb{C}^{n \times n}$. For each eigenvalue $\lambda_i$ of $A$, its algebraic multiplicity $m(\lambda_i)$ and geometric multiplicity $n(\lambda_i)$ sa**tisfy**

$$
1 \leq n(\lambda_i) \leq m(\lambda_i).
$$

# § 1.1.5 對角化

### **Definition (Diagonalizable):**

1. Let $A \in \mathbb{C}^{n \times n}$. If all eigenvalues $\lambda_i, i=1,\ldots,s,$ of $A$ satisfy $n(\lambda_i) = m(\lambda_i)$, then $A$ is called **diagonalizable [可對角化的]**.
    - In the case, we can find $n$ linearly independent eigenvalues for $A$.
2. Let $A \in \mathbb{C}^{n \times n}$. If there is some eigenvalue $\lambda_i$ of $A$ such that $n(\lambda_i) < m(\lambda_i)$, then $A$ is called **degenerated [退化的]** or **non-diagonalizable [不可對角化的]**.
    - In the case, we can find less than $n$ linearly independent eigenvalues for $A$.

### **Theorem (Conditions for Diagonalizable):**

Let $A \in \mathbb{C}^{n \times n}$. The following statements are equivalent:

1. $A$ is diagonalizable.
2. For all eigenvalues $\lambda_i, i=1,\ldots,s,$ of $A$, $n(\lambda_i) = m(\lambda_i)$.
3. There is an **invertible matrix** $V\in \mathbb{C}^{n \times n}$ and **diagonal matrix** $D\in \mathbb{C}^{n \times n}$ such that
    
    $$
    A = VDV^{-1} \; (\Longleftrightarrow V^{-1}AV = D
    \Longleftrightarrow AV= VD, D:\text{ invertible}).
    $$
    

For the expression $A = VDV^{-1}$. By letting $D=\text{diag}( \begin{bmatrix} d_1 &d_2 &\cdots &d_n \end{bmatrix} )$ and $V=\begin{bmatrix} v_1 \mid v_2 \mid \cdots \mid v_n \end{bmatrix}$, we have 

$$
\begin{align*}
A = VDV^{-1}
&\Leftrightarrow AV= VD, \; V\text{: invertible} \\
&\Leftrightarrow A \begin{bmatrix} v_1 \mid v_2 \mid \cdots \mid v_n \end{bmatrix}
= \begin{bmatrix} v_1 \mid v_2 \mid \cdots \mid v_n \end{bmatrix} \cdot 
\text{diag}( \begin{bmatrix} d_1 &d_2 &\cdots &d_n \end{bmatrix} ), \; V\text{: invertible} \\
&\Leftrightarrow 
\begin{bmatrix} Av_1 \mid Av_2 \mid \cdots \mid Av_n \end{bmatrix}
= \begin{bmatrix} d_1v_1 \mid d_2v_2 \mid \cdots \mid d_nv_n \end{bmatrix}, \; V\text{: invertible} \\
&\Leftrightarrow Av_1=dv_1, Av_2=dv_2\cdots, Av_n=dv_n, \; \{v_1,v_2,\cdots, v_n\}\text{: independent} \\
&\Leftrightarrow Av_1=dv_1, Av_2=dv_2\cdots, Av_n=dv_n, \; \{v_1,v_2,\cdots, v_n\}\text{: independent}\\
&\Leftrightarrow \exist \; (d_i, v_i), i=1,\ldots,n, \text{eigenpair of }A, \text{where} \{v_1,v_2,\cdots, v_n\}\text{: independent}.
\end{align*}

$$

### **Definition (Diagonalization):**

Let  $A \in \mathbb{C}^{n \times n}$. Then the process to express $A=VDV^{-1}$ for some invertible $V$ and diagonal matrix $D$ is called the **diagonalization [對角化]** of $A$.

### **Process (To find Diagonalization of a matrix):**

Let $A \in \mathbb{C}^{n \times n}$. 

1. Find the **characteristic polynomial [特徵多項式]** of $A$: $p(\lambda) = \det(\lambda I - A)$.
2. Find all **eigenvalues** $\lambda_i, i=1,\ldots,s,$ of $A$ by factoring $p(\lambda) = \prod\limits_{i=1}^{s} (\lambda - \lambda_i)^{m(\lambda_i)}$.
3. For each eigenvalue $\lambda_i, i=1,\ldots,s,$ find $n(\lambda_i)$’s  **linearly independent  eigenvectors** by finding
    
    $$
    \begin{align*}
    \{ v \neq 0 \mid (A-\lambda_i I) v =0 \} &= \text{null}( A-\lambda_i I )- \{ 0 \}\\
    &= \text{span}( v^{(i)}_1, v^{(i)}_2, \cdots, v^{(i)}_{n(\lambda_i)} ) - \{ 0 \}.
    \end{align*}
    $$
    
4. If there is some eigenvalue $\lambda_i$ of $A$ such that $n(\lambda_i) < m(\lambda_i)$, then $A$ can Not be **diagonalized**. Otherwise, by letting
    
    $$
    \begin{align*}
    V &= \begin{bmatrix} 
    v^{(i)}_1, \cdots, v^{(i)}_{n(\lambda_i)} \mid\ldots \mid
    v^{(s)}_1, \cdots, v^{(s)}_{n(\lambda_s)}
    \end{bmatrix}\\
    D &= \begin{bmatrix} 
    \lambda_1, \cdots, \lambda_1\mid \cdots \mid \lambda_s, \cdots, \lambda_s
    \end{bmatrix}.
    \end{align*}
    $$
    
    Then $A=VDV^{-1}$.
    

**Remark**: Only diagonalizable matrix can be derived the diagonalization $A=VDV^{-1}$, where $V\in \mathbb{C}^{n \times n}$ is **invertible** and $D\in \mathbb{C}^{n \times n}$ is **diagonal**.

# § 1.1.5 Schur 分解

### **Theorem (Schur Lemma; Schur Decomposition):**

1. Let $A \in \mathbb{C}^{n \times n}$. There is an **unitary matrix** $U\in \mathbb{C}^{n \times n}$ and **upper triangular matrix** $R\in \mathbb{C}^{n \times n}$ such that
    
    $$
    A= URU^*.
    $$
    
    - Any **unitary matrix** $U$ satisfies $UU^*=U^*U=I$. Equivalently, $U^{-1}= U^*$.
    - The diagonal elements of $R$ are just eigenvalues of $A$.

### **Theorem (Unitary Diagonalizability for Normal and Hermitian; Orthogonal Diagonalizability for Symmetric Matrices):**

1. Let  $A \in \mathbb{C}^{n \times n}$. Then A is **normal** $( \text{i.e., } AA^*=A^*A)$ $\Leftrightarrow$  There is an **unitary matrix**  $U\in \mathbb{C}^{n \times n}$ and **complex diagonal matrix** $D\in \mathbb{C}^{n \times n}$  such that $A = UDU^*$.
    - It implies that $A$ has  $n$ linearly independent eigenvectors $\{v_1,v_2,\cdots,v_n \}\subseteq \mathbb{C}^n$, which are **orthonormal.**
2. Let  $A \in \mathbb{C}^{n \times n}$. Then $A$ is **Hermitian** $( \text{i.e., } A^*=A)$  $\Leftrightarrow$ there is an **unitary matrix**  $U\in \mathbb{C}^{n \times n}$ and **real diagonal matrix** $D\in \mathbb{R}^{n \times n}$ such that $A = UDU^*$.
    - All eigenvalues of **Hermitian matrices** are real. And the matrix exists eigenvectors  $\{v_1,v_2,\cdots,v_n \}\subseteq \mathbb{C}^n$, which are **orthonormal.**
3. Let  $A \in \mathbb{R}^{n \times n}$. Then $A$ is **symmetric**  $\Leftrightarrow$  there is an **orthogonal matrix** $U\in \mathbb{R}^{n \times n}$ and **real diagonal matrix** $D\in \mathbb{R}^{n \times n}$ such that $A = UDU^*$.
    - All eigenvalues of **symmetric matrices** are real. And the matrix exists eigenvectors  $\{v_1,v_2,\cdots,v_n \}\subseteq \mathbb{R}^n$, which are **orthonormal.**

# § 1.1.6 Jordan Form

### **Theorem (Jordan Form):**

1. [Recall] For any **diagonalizable** $A \in \mathbb{C}^{n \times n}$, we can find **invertible** $V\in \mathbb{C}^{n \times n}$ and **invertible matrix** $D\in \mathbb{C}^{n \times n}$ such that

$$
A= VDV^{-1}.
$$

1. For any **(degenerated)** $A \in \mathbb{C}^{n \times n}$, we can find **invertible** $V\in \mathbb{C}^{n \times n}$ and **Jordan matrix** $J\in \mathbb{C}^{n \times n}$ such that

$$
A= VJV^{-1}.
$$

# § 1.1.7 Real Diagonalization; Real Schur Decomposition; Real Jordan Form

### **Theorem (Real Block Diagonalization; Real Schur Decomposition; Real Jordan Form):**

1. For any  diagonalizable $A \in \mathbb{R}^{n \times n}$, there is an **invertible matrix** $V\in \mathbb{R}^{n \times n}$ and **block diagonalizable matrx [塊對角]** $D_B\in \mathbb{R}^{n \times n}$ such that
    
    $$
    A= VD_BV^{-1}.
    $$
    
2. For any $A \in \mathbb{R}^{n \times n}$, there is an **orthogonal matrix** $U\in \mathbb{R}^{n \times n}$  and **quasi-upper triangular [類上三角]** $\bar{R}\in \mathbb{R}^{n \times n}$ such that
    
    $$
    A= U\bar{R}U^*.
    $$
    
    - An example of quasi-upper triangular:
    
    $$
    \begin{bmatrix}
    \lambda_1 & * & * & * \\
    0 & \lambda_2 & * & * \\
    0 & 0 & \alpha & \beta \\
    0 & 0 & -\beta & \alpha
    \end{bmatrix}
    $$
    
3. For any $A \in \mathbb{R}^{n \times n}$, there is an **invertible matrix** $V\in \mathbb{R}^{n \times n}$  and **real (block) Jordan matrix** $J\in \mathbb{R}^{n \times n}$ such that
    
    $$
    A= VJV^{-1}.
    $$
    
    - An example of real Jordan matrix:
    
    $$
    J = \begin{bmatrix}
    2 & 1 & 0 & 0 \\
    0 & 2 & 0 & 0 \\
    0 & 0 & 3 & 4 \\
    0 & 0 & -4 & 3
    \end{bmatrix}
    $$